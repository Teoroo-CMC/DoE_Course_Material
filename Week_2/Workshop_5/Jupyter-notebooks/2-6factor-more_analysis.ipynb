{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fac9d630",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Teoroo-CMC/DoE_Course_Material/blob/main/Week_2/workshop_4/Jupyter-notebooks/2-6factor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12a3edb8",
   "metadata": {},
   "source": [
    "# More analysis of the six-Factor Full Factorial Design\n",
    "\n",
    "## Introduction\n",
    "Let us continue where we ended in Task 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5829c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import rand, seed\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from matplotlib.pyplot import *\n",
    "\n",
    "seed(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "876ac13c",
   "metadata": {},
   "source": [
    "## Two-Level Six-Factor Full Factorial Design\n",
    "First, let's recover the data from Task 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418791ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Create the inputs:\n",
    "encoded_inputs = list( itertools.product([-1,1],[-1,1],[-1,1],[-1,1],[-1,1],[-1,1]) )\n",
    "\n",
    "# Create the experiment design table:\n",
    "doe=pd.DataFrame(encoded_inputs)\n",
    "doe=doe[doe.columns[::-1]]\n",
    "doe.columns=['x%d'%(i+1) for i in range(6)]\n",
    "doe\n",
    "\n",
    "doe['y1'] = [3.4, 9.7, 7.4, 10.6, 6.5, 7.9, 10.3, 9.5, 14.3, 10.5, 7.8, 17.2, 9.4, 12.1, 9.5, 15.8, 8.3, 8.0, 7.9, 10.7, 7.2, 7.2, 7.9, 10.2, 10.3, 9.9, 7.4, 10.5, 9.6, 15.1, 8.7, 12.1, 12.6, 10.5, 11.3, 10.6, 8.1, 12.5, 11.1, 12.9, 14.6, 12.7, 10.8, 17.1, 13.6, 14.6, 13.3, 14.4, 11.0, 12.5, 8.9, 13.1, 7.6, 8.6, 11.8, 12.4, 13.4, 14.6, 14.9, 11.8, 15.6, 12.8, 13.5, 15.8] \n",
    "doe['y2'] = [15,5,23, 8,20, 9, 13, 5, 23, 1, 11, 5, 15, 8, 15, 1, 22, 8, 16, 7, 25, 5, 17, 8, 10, 3, 22, 6, 24, 4, 10, 5, 32, 10, 28, 18, 22, 31, 17, 16, 38, 12, 34, 19, 12, 14, 25, 16, 31, 14, 23, 23, 28, 20, 18, 11, 39, 30, 31, 6, 33, 23, 31, 11]\n",
    "doe['y3'] = [36, 35, 37, 34, 30, 32, 28, 38, 40, 32, 32, 28, 34, 26,  30, 28, 40, 30, 35, 35, 32, 35, 36, 32, 20, 35, 35, 28, 27, 36, 36, 35, 32, 34, 30, 24, 30, 20, 32, 25, 20, 20, 22, 35, 26, 15, 19, 24, 22, 23, 22, 18, 20, 20, 20, 36, 20, 11, 20, 35, 16, 32, 20, 20]\n",
    "print(doe[['y1','y2','y3']])\n",
    "\n",
    "# Defining Variables and Variable Labels\n",
    "\n",
    "labels = {}\n",
    "labels[1] = ['x1','x2','x3','x4','x5','x6']\n",
    "for i in [2,3,4,5,6]:\n",
    "    labels[i] = list(itertools.combinations(labels[1], i))\n",
    "\n",
    "obs_list = ['y1','y2','y3']\n",
    "\n",
    "for k in labels.keys():\n",
    "    print(str(k) + \" : \" + str(labels[k]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2a51eb8",
   "metadata": {},
   "source": [
    "## Utilizing Degrees of Freedom\n",
    "Our very expensive, 64-experiment full factorial design (the data for which maps  (x$_1$,x$_2$,…,x$_6$) to (y$_1$,y$_2$,y$_3$) gives us 64 data points, and 64 degrees of freedom. What we do with those 64 degrees of freedom is up to us.\n",
    "\n",
    "We could fit an empirical model, or response surface, that has 64 independent parameters, and account for many of the high-order interaction terms - all the way up to six-variable interaction effects. However, high-order effects are rarely important, and are a waste of our degrees of freedom.\n",
    "\n",
    "Alternatively, we can fit an empirical model with fewer coefficients, using up fewer degrees of freedom, and use the remaining degrees of freedom to characterize the error introduced by our approximate model.\n",
    "\n",
    "To describe a model with the 3 variables listed above and no other variable interaction effects would use only 3 degrees of freedom, plus 1 degree of freedom for the constant term, leaving 60 degrees of freedom available to quantify error, attribute variance, etc.\n",
    "\n",
    "Our goal is to use least squares to compute model equations for  (y$_1$,y$_2$,y$_3$)\n",
    "  as functions of  (x$_1$,x$_4$,x$_6$)\n",
    " ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83796ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlabs = ['x1','x4','x6']\n",
    "ylabs = ['y1','y2','y3']\n",
    "ls_data = doe[xlabs+ylabs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba02dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "x = ls_data[xlabs]\n",
    "x = sm.add_constant(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a48d58a6",
   "metadata": {},
   "source": [
    "The first ordinary least squares linear model is created to predict values of the first variable,  y$_1$, as a function of each of our input variables, the list of which are contained in the xlabs variable. When we perform the linear regression fitting, we see much of the same information that we found in the prior two-level three-factor full factorial design, but here, everything is done automatically.\n",
    "\n",
    "The model is linear, meaning it's fitting the coefficients of the function:\n",
    "\\begin{equation}\n",
    "\\hat{y}=a_0+a_1x_1+a_2x_2+a_3+x_3+a_4x_4+a_5x_5+a_6x_6\n",
    "\\end{equation} \n",
    "(here, the variables y and x are vectors, with one component for each response; in our case, they are three-dimensional vectors.)\n",
    "\n",
    "Because there are 64 observations and 7 coefficients, the 57 extra observations give us extra degrees of freedom with which to assess how good the model is. That analysis can be done with an ordinary least squares (OLS) model, available through the statsmodel library in Python."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e342cff",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares Regression Model\n",
    "This built-in OLS model will fit an input vector  (x$_1$,x$_2$,x$_3$,x$_4$,x$_5$,x$_6$)\n",
    "  to an output vector  (y$_1$,y$_2$,y$_3$)\n",
    "  using a linear model; the OLS model is designed to fit the model with more observations than coefficients, and utilize the remaining data to quantify the fit of the model.\n",
    "\n",
    "Let's run through one of these, and analyze the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e068c053",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = ls_data['y1']\n",
    "est1 = sm.OLS(y1,x).fit()\n",
    "print(est1.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa0db35e",
   "metadata": {},
   "source": [
    "The StatsModel OLS object prints out quite a bit of useful information, in a nicely-formatted table. Starting at the top, we see a couple of important pieces of information: specifically, the name of the dependent variable (the response) that we're looking at, the number of observations, and the number of degrees of freedom.\n",
    "\n",
    "We can see an  R$^2$ statistic, which indicates how well this data is fit with our linear model, and an adjusted R$^2$\n",
    "  statistic, which accounts for the large nubmer of degrees of freedom. While an adjusted R$^2$\n",
    "  of 0.565 is not great, we have to remember that this linear model is trying to capture a wealth of complexity in six coefficients. Furthermore, the adjusted  R$^2$\n",
    "  value is too broad to sum up how good our model actually is.\n",
    "\n",
    "The table in the middle is where the most useful information is located. The coef column shows the coefficients  a$_0$,a$_1$,a$_2$,…\n",
    "  for the model equation:\n",
    "\\begin{equation}\n",
    "\\hat{y}=a_0+a_1x_1+a_2x_2+a_3+x_3+a_4x_4+a_5x_5+a_6x_6\n",
    "\\end{equation}\n",
    " \n",
    "Using the extra degrees of freedom, an estime s$^2$\n",
    "  of the variance in the regression coefficients is also computed, and reported in the the std err column. Each linear term is attributed the same amount of variance,  ±0.24\n",
    " ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1010681",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = ls_data['y2']\n",
    "est2 = sm.OLS(y2,x).fit()\n",
    "print(est2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf27221",
   "metadata": {},
   "outputs": [],
   "source": [
    "y3 = ls_data['y3']\n",
    "est3 = sm.OLS(y3,x).fit()\n",
    "print(est3.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf314f5a",
   "metadata": {},
   "source": [
    "## Quantifying Model Goodness-of-Fit\n",
    "We can now use these linear models to evaluate each set of inputs and compare the model response  $\\hat{y}$ to the actual observed response y. What we would expect to see, if our model does an adequate job of representing the underlying behavior of the model, is that in each of the 64 experiments, the difference between the model prediction M and the measured data d, defined as the residual r, r=|d−M|, should be comparable across all experiments. If the residuals appear to have functional dependence on the input variables, it is an indication that our model is missing important effects and needs more or different terms. The way we determine this, mathematically, is by looking at a quantile-quantile plot of our errors (that is, a ranked plot of our error magnitudes).\n",
    "\n",
    "If the residuals are normally distributed, they will follow a straight line; if the plot shows the data have significant wiggle and do not follow a line, it is an indication that the errors are not normally distributed, and are therefore skewed (indicating terms missing from our OLS model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d581b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from matplotlib.pyplot import *\n",
    "\n",
    "# Quantify goodness of fit\n",
    "\n",
    "fig = figure(figsize=(14,4))\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax3 = fig.add_subplot(133)\n",
    "\n",
    "r1 = y1 - est1.predict(x)\n",
    "r2 = y2 - est2.predict(x)\n",
    "r3 = y3 - est3.predict(x)\n",
    "\n",
    "stats.probplot(r1, dist=\"norm\", plot=ax1)\n",
    "ax1.set_title('Residuals, y1')\n",
    "\n",
    "stats.probplot(r2, dist=\"norm\", plot=ax2)\n",
    "ax2.set_title('Residuals, y2')\n",
    "\n",
    "stats.probplot(r3, dist=\"norm\", plot=ax3)\n",
    "ax3.set_title('Residuals, y3')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35d76204",
   "metadata": {},
   "source": [
    "Determining whether significant trends are being missed by the model depends on how many points deviate from the red line, and how significantly. If there is a single point that deviates, it does not necessarily indicate a problem; but if there is significant wiggle and most points deviate significantly from the red line, it means that there is something about the relationship between the inputs and the outputs that our model is missing.\n",
    "\n",
    "There are only a few points deviating from the red line. We saw from the effect quantile for y$_3$\n",
    "  that there was an interaction variable that was important to modeling the response  y$_3$\n",
    " , and it is likely this interaction that is leading to noise at the tail end of these residuals. This indicates residual errors (deviations of the model from data) that do not follow a natural, normal distribution, which indicates there is a pattern in the deviations - namely, the interaction effect.\n",
    "\n",
    "The conclusion about the error from the quantile plots above is that there are only a few points deviation from the line, and no particularly significant outliers. Our model can use some improvement, but it's a pretty good first-pass model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6791eff8",
   "metadata": {},
   "source": [
    "## Distribution of Error\n",
    "Another thing we can look at is the normalized error: what are the residual errors (differences between our model prediction and our data)? How are their values distributed?\n",
    "\n",
    "A kernel density estimate (KDE) plot, which is a smoothed histogram, shows the probability distribution of the normalized residual errors. As expected, they're bunched pretty close to zero. There are some bumps far from zero, corresponding to the outliers on the quantile-quantile plot of the errors above. However, they're pretty close to randomly distributed, and therefore it doesn't look like there is any systemic bias there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad8862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "fig = figure(figsize=(10,12))\n",
    "ax1 = fig.add_subplot(311)\n",
    "ax2 = fig.add_subplot(312)\n",
    "ax3 = fig.add_subplot(313)\n",
    "axes = [ax1,ax2,ax3]\n",
    "\n",
    "colors = sns.xkcd_palette([\"windows blue\", \"amber\", \"faded green\", \"dusty purple\",\"aqua blue\"])\n",
    "\n",
    "#resids = [r1, r2, r3]\n",
    "normed_resids = [r1/y1, r2/y2, r3/y3]\n",
    "\n",
    "for (dataa, axx, colorr) in zip(normed_resids,axes,colors):\n",
    "    sns.kdeplot(dataa, ax=axx, color=colorr, shade=True, alpha=0.5);\n",
    "\n",
    "ax1.set_title('Probability Distribution: Normalized Residual Error, y1')\n",
    "ax2.set_title('Normalized Residual Error, y2')\n",
    "ax3.set_title('Normalized Residual Error, y3')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23f6b9e1",
   "metadata": {},
   "source": [
    "Note that in these figures, the bumps at extreme value are caused by the fact that the interval containing the responses includes 0 and values close to 0, so the normalization factor is very tiny, leading to large values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f3e11e8",
   "metadata": {},
   "source": [
    "## Analysis of Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1e069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlabs = ['x1','x4', 'x6']\n",
    "\n",
    "doe.groupby(xlabs)[ylabs].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bcb078",
   "metadata": {},
   "outputs": [],
   "source": [
    "doe.groupby(xlabs)[ylabs].var()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "427dd818",
   "metadata": {},
   "source": [
    "## Distributions of Variance\n",
    "We can convert these dataframes of averages, variances, and counts into data for plotting. For example, if we want to make a histogram of every value in the groupby dataframe, we can use the .values method, so that this:\n",
    "´´´\n",
    "doe.gorupby(xlabs)[ylabs].mean()\n",
    "´´´\n",
    "becomes this:\n",
    "\n",
    "doe.groupby(xlabs)[ylabs].mean().values\n",
    "\n",
    "\n",
    "This  M×N\n",
    "  array can then be flattened into a vector using the ravel() method from numpy:\n",
    "\n",
    "np.ravel( doe.groupby(xlabs)[ylabs].mean().values )\n",
    "\n",
    "The resulting data can be used to generate histograms, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc546d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of means of response values, grouped by xlabs\n",
    "\n",
    "xlabs = ['x1','x4','x6']\n",
    "\n",
    "print(\"Grouping responses by %s\"%( \"-\".join(xlabs) ))\n",
    "\n",
    "dat = np.ravel(doe.groupby(xlabs)[ylabs].mean().values) / np.ravel(doe.groupby(xlabs)[ylabs].var().values)\n",
    "\n",
    "hist(dat, 10, color=colors[3]);\n",
    "xlabel(r'Relative Variance ($\\mu$/$\\sigma^2$)')\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af94dd62",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Histogram of variances of response values, grouped by xlabs\n",
    "\n",
    "print(\"Grouping responses by %s\"%( \"-\".join(xlabs) ))\n",
    "\n",
    "dat = np.ravel(doe.groupby(xlabs)['y1'].var().values)\n",
    "\n",
    "hist(dat, color=colors[4])\n",
    "xlabel(r'Variance in $y_{1}$ Response')\n",
    "ylabel(r'Frequency')\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab27bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of variances of response values, grouped by xlabs\n",
    "\n",
    "print(\"Grouping responses by %s\"%( \"-\".join(xlabs) ))\n",
    "\n",
    "dat = np.ravel(doe.groupby(xlabs)['y2'].var().values)\n",
    "\n",
    "hist(dat, color=colors[4])\n",
    "xlabel(r'Variance in $y_{2}$ Response')\n",
    "ylabel(r'Frequency')\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b194478e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of variances of response values, grouped by xlabs\n",
    "\n",
    "print(\"Grouping responses by %s\"%( \"-\".join(xlabs) ))\n",
    "\n",
    "dat = np.ravel(doe.groupby(xlabs)['y3'].var().values)\n",
    "\n",
    "hist(dat, color=colors[4])\n",
    "xlabel(r'Variance in $y_{3}$ Response')\n",
    "ylabel(r'Frequency')\n",
    "show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8012c3c5",
   "metadata": {},
   "source": [
    "This reflects that y1 is best described by the chosen parameters... "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aea5e9f9",
   "metadata": {},
   "source": [
    "## Residual vs. Response Plots\n",
    "Another thing we can do, to look for uncaptured effects, is to look at our residuals vs.  $\\hat{y}$. This is a further effort to look for underlying functional relationships between  $\\hat{y}$\n",
    "  and the residuals, which would indicate that our system exhibits behavior not captured by our linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd40dbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal plot of residuals\n",
    "\n",
    "fig = figure(figsize=(14,4))\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax3 = fig.add_subplot(133)\n",
    "\n",
    "ax1.plot(y1,r1,'o',color=colors[0])\n",
    "ax1.set_xlabel('Response value $y_1$')\n",
    "ax1.set_ylabel('Residual $r_1$')\n",
    "\n",
    "ax2.plot(y2,r2,'o',color=colors[1])\n",
    "ax2.set_xlabel('Response value $y_2$')\n",
    "ax2.set_ylabel('Residual $r_2$')\n",
    "ax2.set_title('Response vs. Residual Plots')\n",
    "\n",
    "ax3.plot(y1,r1,'o',color=colors[2])\n",
    "ax3.set_xlabel('Response value $y_3$')\n",
    "ax3.set_ylabel('Residual $r_3$')\n",
    "\n",
    "show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7af43a85",
   "metadata": {},
   "source": [
    "Notice that each plot is trending up and to the right - indicative of an underlying trend that our model  $\\hat{y}$\n",
    "  is not capturing. The trend is relatively weak, however, indicating that our linear model does a good job of capturing most of the relevant effects of this system.\n",
    "\n",
    "## Discussion\n",
    "The analysis shows that there are some higher-order or nonlinear effects in the system that a purely linear model does not account for. Next steps would involve adding higher order points for a quadratic or higher order polynomial model to gather additional data to fit the higher-degree models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
